{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axyZYTozIAOb",
        "outputId": "217bd772-5657-4378-96ba-78bca7ea4839"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzh2xESKHvgW",
        "outputId": "cfe255fa-a3dc-4eda-c581-91fadbcc4062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thinking content: <think>\n",
            "Okay, the user wants a short introduction to a large language model. Let me start by recalling what I know. Large language models are AI systems designed to understand and generate human language. They have a huge dataset, so they can learn from a lot of text.\n",
            "\n",
            "I should mention their ability to understand and generate natural language, which makes them useful in various fields like customer service, content creation, and research. Also, their training data is vast, so they can handle complex tasks. \n",
            "\n",
            "Wait, maybe I should highlight their adaptability and how they can improve with more data. Should I include examples of their applications? Like helping with writing, answering questions, or even creative tasks. \n",
            "\n",
            "I need to keep it concise but informative. Avoid jargon, make sure it's clear and straightforward. Let me check if I'm missing any key points. Oh, also mention that they're trained on massive amounts of text, which enhances their performance. \n",
            "\n",
            "Putting it all together, the introduction should be friendly and highlight the main features and benefits. Make sure it's under a sentence or two. Alright, that should cover it.\n",
            "</think>\n",
            "content: A large language model (LLM) is an AI system capable of understanding and generating human language. It is trained on vast datasets to learn patterns and nuances in text, enabling it to assist in tasks like writing, answering questions, and creating content. These models are highly adaptable and can improve with additional data.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=32768\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content)\n",
        "print(\"content:\", content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class QwenChatbot:\n",
        "    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.history = []\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
        "        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n",
        "        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Update history\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        return response\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot = QwenChatbot()\n",
        "\n",
        "    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n",
        "    user_input_1 = \"How many r's in strawberries?\"\n",
        "    print(f\"User: {user_input_1}\")\n",
        "    response_1 = chatbot.generate_response(user_input_1)\n",
        "    print(f\"Bot: {response_1}\")\n",
        "    print(\"----------------------\")\n",
        "\n",
        "    # Second input with /no_think\n",
        "    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n",
        "    print(f\"User: {user_input_2}\")\n",
        "    response_2 = chatbot.generate_response(user_input_2)\n",
        "    print(f\"Bot: {response_2}\")\n",
        "    print(\"----------------------\")\n",
        "\n",
        "    # Third input with /think\n",
        "    user_input_3 = \"Really? /think\"\n",
        "    print(f\"User: {user_input_3}\")\n",
        "    response_3 = chatbot.generate_response(user_input_3)\n",
        "    print(f\"Bot: {response_3}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUBfSAxsKCYQ",
        "outputId": "34dcfc7b-9797-425f-a7a1-05c0a07c4dfe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: How many r's in strawberries?\n",
            "Bot: <think>\n",
            "Okay, the user is asking how many r's are in the word \"strawberries\". Let me break this down. First, I need to check each letter in the word and count the number of times 'r' appears. Let me write out the word: S-T-R-A-W-B-E-R-R-I-N-G-S. Now, looking at each letter:\n",
            "\n",
            "1. S - no r\n",
            "2. T - no r\n",
            "3. R - one r\n",
            "4. A - no r\n",
            "5. W - no r\n",
            "6. B - no r\n",
            "7. E - no r\n",
            "8. R - another r\n",
            "9. R - third r\n",
            "10. I - no r\n",
            "11. N - no r\n",
            "12. G - no r\n",
            "\n",
            "So that's three 'r's. Wait, let me double-check to make sure I didn't miss any. The word is \"strawberries\". Let me spell it again: S-T-R-A-W-B-E-R-R-I-N-G-S. Yep, that's three 'r's. So the answer should be three. I should present this clearly to the user.\n",
            "</think>\n",
            "\n",
            "There are **3 r's** in the word \"strawberries\".  \n",
            "\n",
            "**Answer:** 3\n",
            "----------------------\n",
            "User: Then, how many r's in blueberries? /no_think\n",
            "Bot: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "There are **3 r's** in the word **blueberries**.  \n",
            "\n",
            "**Answer:** 3\n",
            "----------------------\n",
            "User: Really? /think\n",
            "Bot: <think>\n",
            "Okay, the user just asked, \"Really?\" and I need to respond. Let me think about this.\n",
            "\n",
            "First, they previously asked about the number of 'r's in strawberries and blueberries. Both had 3 r's. Now, they're confirming that the answer is really correct. But maybe they want to check if there's a mistake or if they're testing if I can confirm the answer.\n",
            "\n",
            "I should acknowledge their confirmation. Maybe say something like \"Yes, that's correct!\" to confirm. Also, perhaps offer to check again if they need help. But since they just said \"Really?\", maybe just a simple affirmation.\n",
            "\n",
            "Wait, the user might be checking if I'm correct. So I should respond with a positive affirmation and maybe ask if they need more help. But since they just said \"Really?\", maybe just confirm and thank them. Let me make sure the answer is accurate. Yes, both strawberries and blueberries have three r's. So the answer is correct. Just need to present it clearly.\n",
            "</think>\n",
            "\n",
            "Yes, that's correct!  \n",
            "\n",
            "**Answer:** 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPWE2oWgKCSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uYtldJIXKCGV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}